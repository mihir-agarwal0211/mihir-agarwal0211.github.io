<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <style>@media(min-width:1200px){.container{max-width:1400px!important}}</style> </head> <body> <div class="row justify-content-center"> <div class="col-lg-10 col-md-11"> <table> <tbody> <tr> <td> <strong>Status:</strong> Ongoing Research &amp; Implementation Repo</td> <td> <strong>Stack:</strong> PyTorch, Hugging Face, LangChain, FAISS</td> </tr> </tbody> </table> <h3 id="motivation">Motivation</h3> <p>Modern NLP relies heavily on abstraction layers. To master the fundamentals of Large Language Models (LLMs), I implemented core deep learning architectures from scratch in <strong>PyTorch</strong>. This project serves as a testbed for benchmarking attention mechanisms, sequence modeling techniques, and retrieval strategies.</p> <hr> <h3 id="sequence-modeling-rnn--lstm">1. Sequence Modeling (RNN &amp; LSTM)</h3> <p>Implemented recurrent architectures to analyze the <strong>Vanishing Gradient Problem</strong> and long-term dependency retention.</p> <ul> <li> <strong>RNN:</strong> Built a vanilla RNN from scratch to process character-level text generation.</li> <li> <strong>LSTM:</strong> Implemented the gate mechanisms (Forget, Input, Output) manually to demonstrate superior gradient flow compared to RNNs.</li> <li> <strong>Metric:</strong> Compared Perplexity (PPL) on the Penn Treebank dataset.</li> </ul> <h3 id="the-transformer--attention">2. The Transformer &amp; Attention</h3> <p>Replicated the “Attention Is All You Need” architecture without using <code class="language-plaintext highlighter-rouge">nn.Transformer</code>.</p> <ul> <li> <strong>Self-Attention:</strong> Implemented Scaled Dot-Product Attention matrices to visualize token relationships.</li> <li> <strong>Multi-Head Attention:</strong> Engineered parallel attention heads to capture different linguistic subspaces.</li> <li> <strong>Positional Encoding:</strong> Added sinusoidal embeddings to inject sequence order into the permutation-invariant architecture.</li> </ul> <h3 id="bert--transfer-learning">3. BERT &amp; Transfer Learning</h3> <ul> <li> <strong>Fine-Tuning:</strong> Fine-tuned a pre-trained <strong>BERT-base</strong> model on the GLUE benchmark (SST-2) for sentiment analysis.</li> <li> <strong>Optimization:</strong> Utilized AdamW optimizer and linear learning rate schedulers to prevent catastrophic forgetting.</li> </ul> <h3 id="retrieval-augmented-generation-rag">4. Retrieval-Augmented Generation (RAG)</h3> <p>Built a modular RAG pipeline to ground LLM responses in external data.</p> <ul> <li> <strong>Ingestion:</strong> Chunked PDF documents using recursive character splitters.</li> <li> <strong>Embedding:</strong> Generated vector embeddings using <strong>OpenAI/HuggingFace</strong> models.</li> <li> <p><strong>Retrieval:</strong> Implemented similarity search using <strong>FAISS</strong> (Facebook AI Similarity Search) to retrieve Top-K relevant contexts for the generator.</p> <p>&lt;/div&gt;</p> </li> </ul> </div> <div class="row justify-content-center mt-5 mb-5"> <div class="col-md-6 text-center"> <a href="https://github.com/mihir-agarwal0211" target="_blank" class="btn btn-dark btn-lg z-depth-1" rel="external nofollow noopener"> <i class="fab fa-github mr-2"></i> View Source Code </a> </div> </div> </div> </body> </html>